{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /probability/inference/jax/2023/03/26/numpyro-log-joint\n",
    "badges: false\n",
    "categories:\n",
    "- probability\n",
    "- inference\n",
    "- jax\n",
    "date: '2023-03-26'\n",
    "output-file: 2023-03-26-numpyro-log-joint.html\n",
    "title: Translating a Model into a Log Joint Probability\n",
    "toc: false\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In probabilistic programming languages (PPLs), one needs to compute the joint probability (often unnormalized) of values and observed variables under a generative model to perform approximate inference. However, given a model in the form of a Python function, how does one translate this function (model) into a log joint probability? The objective of this blog is to better understand how modern PPLs, in particular NumPyro, performs this translation in a dynamic way, i.e., the functions for performing this translation can handle a variety of models defined by the user."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "The example `zero_inflated_poisson.py` from the NumPyro [docs](https://num.pyro.ai/en/stable/examples/zero_inflated_poisson.html) will be used. In this example, the authors model and predict how many fish are caught by visitors in a state park. Many groups of visitors catch zero fish, either because they did not fish at all or because they were unlucky. They explicitly model this bimodal behavior (zero versus non-zero) and ascertain which variables contribute to each behavior. The authors answer this question by fitting a zero-inflated poisson regression model. We will use NUTs as the inference method to understand the model translation.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. Define model using NumPyro primitives\n",
    "2. Construct a kernel for inference and feed model into kernel\n",
    "3. Perform inference using MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax.scipy as jsp\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, SVI, Predictive, Trace_ELBO, autoguide\n",
    "from numpyro.infer import util\n",
    "\n",
    "matplotlib.use(\"Agg\")  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y):\n",
    "    D_X = X.shape[1]\n",
    "    b1 = numpyro.sample(\"b1\", dist.Normal(0.0, 1.0).expand([D_X]).to_event(1))\n",
    "    b2 = numpyro.sample(\"b2\", dist.Normal(0.0, 1.0).expand([D_X]).to_event(1))\n",
    "\n",
    "    q = jsp.special.expit(jnp.dot(X, b1[:, None])).reshape(-1)\n",
    "    lam = jnp.exp(jnp.dot(X, b2[:, None]).reshape(-1))\n",
    "\n",
    "    with numpyro.plate(\"obs\", X.shape[0]):\n",
    "        numpyro.sample(\"Y\", dist.ZeroInflatedPoisson(gate=q, rate=lam), obs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcmc(model, args, X, Y):\n",
    "    kernel = NUTS(model)\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=args.num_warmup,\n",
    "        num_samples=args.num_samples,\n",
    "        num_chains=args.num_chains,\n",
    "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
    "    )\n",
    "    mcmc.run(PRNGKey(1), X, Y)\n",
    "    mcmc.print_summary()\n",
    "    return mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def main(args):\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # prepare dataset\n",
    "    df = pd.read_stata(\"http://www.stata-press.com/data/r11/fish.dta\")\n",
    "    df[\"intercept\"] = 1\n",
    "    cols = [\"livebait\", \"camper\", \"persons\", \"child\", \"intercept\"]\n",
    "\n",
    "    mask = np.random.randn(len(df)) < args.train_size\n",
    "    df_train = df[mask]\n",
    "    df_test = df[~mask]\n",
    "    X_train = jnp.asarray(df_train[cols].values)\n",
    "    y_train = jnp.asarray(df_train[\"count\"].values)\n",
    "    X_test = jnp.asarray(df_test[cols].values)\n",
    "    y_test = jnp.asarray(df_test[\"count\"].values)\n",
    "\n",
    "    print(\"run MCMC.\")\n",
    "    posterior_samples = run_mcmc(model, args, X_train, y_train)\n",
    "\n",
    "    predictive = Predictive(model, posterior_samples=posterior_samples)\n",
    "    predictions = predictive(PRNGKey(1), X=X_test, Y=None)\n",
    "    mcmc_predictions = jnp.rint(predictions[\"Y\"].mean(0))\n",
    "\n",
    "    print(\n",
    "        \"MCMC RMSE: \",\n",
    "        mean_squared_error(np.asarray(y_test), np.asarray(mcmc_predictions), squared=False),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "parser = argparse.ArgumentParser(\"Zero-Inflated Poisson Regression\")\n",
    "parser.add_argument(\"--seed\", nargs=\"?\", default=42, type=int)\n",
    "parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=2000, type=int)\n",
    "parser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\n",
    "parser.add_argument(\"--num-chains\", nargs=\"?\", default=1, type=int)\n",
    "parser.add_argument(\"--num-data\", nargs=\"?\", default=100, type=int)\n",
    "parser.add_argument(\"--maxiter\", nargs=\"?\", default=5000, type=int)\n",
    "parser.add_argument(\"--train-size\", nargs=\"?\", default=0.8, type=float)\n",
    "parser.add_argument(\"--device\", default=\"cpu\", type=str, help='use \"cpu\" or \"gpu\".')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "numpyro.set_platform(args.device)\n",
    "numpyro.set_host_device_count(args.num_chains)\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the kernel\n",
    "\n",
    "First, we initialize the NUTS kernel with the model. The word _kernel_ is used in a wide range of fields ranging from probabilistic programming, statistics, and deep learning. In PPLs, the name kernel is typically used to define the interface with the sampling algorithm. In this case, we have initialized a NUTS kernel with our model, and this kernel will allow us to interface our model with the underlying HMC sampling variant NUTS. \n",
    "\n",
    "But, the sampling algorithm can't simply interface with a Python function. Our model, in the form of a Python function, needs to be translated into a joint log density function and used as input into the sampler. Here, this is where NumPyro performs a series of steps to perform this translation.  \n",
    "\n",
    "When we \"feed\" the model into the `NUTS` class an `initialize_model` utility function is called. This function calls various helper functions such as `get_potential_fn` and `find_valid_initial_params` to return a tuple of (`init_params_info`, `potential_fn`, `postprocess_fn`, `model_trace`). Here, we are interested in `initialize_model` and `get_potential_fn`.\n",
    "\n",
    "The graph of function calls looks like: initialize model $\\leftrightarrow$ get potential fn $\\leftrightarrow$ potential energy $\\leftrightarrow$ log density where each function being called is also returning an object. Below, the sequential order of functions calls are described."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize_model\n",
    "\n",
    "`initialize_model` is a function that returns a tuple of objects and values used as input into the HMC algorithm. At a high level, our model and data are passed into the `initialize_model` function to intialize the model to some values using the observed data and `numpyro.sample` statements. This initialization allows us to perform inference with NUTS. Below, the various helper functions that are called within this function are described as these helpers constitute where the majority of our interest lies regarding translating a model into a log joint probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_potential_fn\n",
    "\n",
    "Inside of `intialize_model`, the function `get_potential_fn` is called. Given a model with Pyro primitives, this Python function returns another function which, given unconstrained parameters, evaluates the potential energy (negative log joint density). In addition, this returns a function to transform unconstrained values at sample sites to constrained values within their respective support. \n",
    "\n",
    "The interesting parts here are the _evaluation of potential energy_ and the _returns a function_. First, we focus on the function `potential_energy` to evaluate the potential energy. Later, we then return to the `potential_fn` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def get_potential_fn(\n",
    "    model,\n",
    "    inv_transforms,\n",
    "    *,\n",
    "    enum=False,\n",
    "    replay_model=False,\n",
    "    dynamic_args=False,\n",
    "    model_args=(),\n",
    "    model_kwargs=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    (EXPERIMENTAL INTERFACE) Given a model with Pyro primitives, returns a\n",
    "    function which, given unconstrained parameters, evaluates the potential\n",
    "    energy (negative log joint density). In addition, this returns a\n",
    "    function to transform unconstrained values at sample sites to constrained\n",
    "    values within their respective support.\n",
    "\n",
    "    :param model: Python callable containing Pyro primitives.\n",
    "    :param dict inv_transforms: dictionary of transforms keyed by names.\n",
    "    :param bool enum: whether to enumerate over discrete latent sites.\n",
    "    :param bool replay_model: whether we need to replay model in\n",
    "        `postprocess_fn` to obtain `deterministic` sites.\n",
    "    :param bool dynamic_args: if `True`, the `potential_fn` and\n",
    "        `constraints_fn` are themselves dependent on model arguments.\n",
    "        When provided a `*model_args, **model_kwargs`, they return\n",
    "        `potential_fn` and `constraints_fn` callables, respectively.\n",
    "    :param tuple model_args: args provided to the model.\n",
    "    :param dict model_kwargs: kwargs provided to the model.\n",
    "    :return: tuple of (`potential_fn`, `postprocess_fn`). The latter is used\n",
    "        to constrain unconstrained samples (e.g. those returned by HMC)\n",
    "        to values that lie within the site's support, and return values at\n",
    "        `deterministic` sites in the model.\n",
    "    \"\"\"\n",
    "    if dynamic_args:\n",
    "        potential_fn = partial(\n",
    "            _partial_args_kwargs, partial(potential_energy, model, enum=enum)\n",
    "        )\n",
    "        if replay_model:\n",
    "            # XXX: we seed to sample discrete sites (but not collect them)\n",
    "            model_ = seed(model.fn, 0) if enum else model\n",
    "            postprocess_fn = partial(\n",
    "                _partial_args_kwargs,\n",
    "                partial(constrain_fn, model, return_deterministic=True),\n",
    "            )\n",
    "        else:\n",
    "            postprocess_fn = partial(\n",
    "                _drop_args_kwargs, partial(transform_fn, inv_transforms)\n",
    "            )\n",
    "    else:\n",
    "        model_kwargs = {} if model_kwargs is None else model_kwargs\n",
    "        potential_fn = partial(\n",
    "            potential_energy, model, model_args, model_kwargs, enum=enum\n",
    "        )\n",
    "        if replay_model:\n",
    "            model_ = seed(model.fn, 0) if enum else model\n",
    "            postprocess_fn = partial(\n",
    "                constrain_fn,\n",
    "                model_,\n",
    "                model_args,\n",
    "                model_kwargs,\n",
    "                return_deterministic=True,\n",
    "            )\n",
    "        else:\n",
    "            postprocess_fn = partial(transform_fn, inv_transforms)\n",
    "\n",
    "    print(f\"potential_fn: {potential_fn}\")\n",
    "    return potential_fn, postprocess_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## potential_energy\n",
    "\n",
    "Computes potential energy (negative joint log density) of a model given unconstrained parameters. Under the hood, NumPyro will transform these unconstrained parameters to the values belonging to the supports of the corresponding priors in the model. To compute the potential energy, this function calls a `log_density` function that computes the log of joint density for the model given the latent values (parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def potential_energy(model, model_args, model_kwargs, params, enum=False):\n",
    "    \"\"\"\n",
    "    (EXPERIMENTAL INTERFACE) Computes potential energy of a model given unconstrained params.\n",
    "    Under the hood, we will transform these unconstrained parameters to the values\n",
    "    belong to the supports of the corresponding priors in `model`.\n",
    "\n",
    "    :param model: a callable containing NumPyro primitives.\n",
    "    :param tuple model_args: args provided to the model.\n",
    "    :param dict model_kwargs: kwargs provided to the model.\n",
    "    :param dict params: unconstrained parameters of `model`.\n",
    "    :param bool enum: whether to enumerate over discrete latent sites.\n",
    "    :return: potential energy given unconstrained parameters.\n",
    "    \"\"\"\n",
    "    if enum:\n",
    "        from numpyro.contrib.funsor import log_density as log_density_\n",
    "    else:\n",
    "        log_density_ = log_density\n",
    "\n",
    "    substituted_model = substitute(\n",
    "        model, substitute_fn=partial(_unconstrain_reparam, params)\n",
    "    )\n",
    "    # no param is needed for log_density computation because we already substitute\n",
    "    log_joint, model_trace = log_density_(\n",
    "        substituted_model, model_args, model_kwargs, {}\n",
    "    )\n",
    "    print(f\"-log_joint: {log_joint}\")\n",
    "    return -log_joint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our NumPyro model, data (model_args), and initialized parameters (using numpyro.sample), the potential energy (negative log joint density) is the following output:\n",
    "\n",
    "```\n",
    "-log_joint: Traced<ConcreteArray([ 15.773586   15.796449   15.68768    17.106882   16.307858   15.10714\n",
    "  16.277817   16.401972   17.409088   15.488239   17.44108    20.011204\n",
    "  15.796449   16.401972   17.409088   15.488239   32.46398    16.306961\n",
    "  18.321064   15.776845   16.432753   41.972443   16.90719    17.324219\n",
    "  15.487675   15.68768    15.10714    15.68768    16.96474    61.152782\n",
    "  16.319      32.46398    56.631355   16.733091   44.390583   15.796449\n",
    "  31.771408   15.9031725  17.999393   15.929144   15.796449   15.776845\n",
    "  25.234818   15.487675   32.46398    15.776845   22.136528   16.745249\n",
    "  15.796449   15.796449   61.152782   17.459694   15.776845   39.30664\n",
    "  31.771408   17.106882   15.796449   15.776845   16.836826   16.90372\n",
    "  15.565512   15.266311   15.796449   15.487675   25.503807   66.416145\n",
    "  42.01054    15.68768    16.438005   35.528217   16.401972  275.8356\n",
    "  15.488239   46.813717   18.31475    42.01054    15.929144   16.733091\n",
    "  15.929144   18.632296   16.553946   22.139755   16.879503   16.253452\n",
    "  15.929144   16.68712    16.90719    17.409088   16.306961   17.900412\n",
    "  72.883484   20.984446   17.080605   15.68768    15.266311   17.459694\n",
    "  15.487675   17.409088  221.04407    15.487675   15.68768    15.68768\n",
    "  15.903938   17.608683   17.233418   16.945618   17.102604   16.230682\n",
    "  16.401972   20.437622   16.307858   15.776845   66.416145   22.85551\n",
    "  17.459694   15.266729   18.263693   16.733091   15.68768    16.69443\n",
    "  17.767635   16.892221   16.277817   16.699389   15.796449   15.776845\n",
    "  15.68768    17.459694   18.93738    16.401972   41.471767   15.796449\n",
    "  82.16476    16.664154   15.68768    17.409088   17.106882   15.488239\n",
    "  15.266729   17.917303   26.629543   21.383934   18.279554   15.929144\n",
    "  16.90719    38.06461    16.673416   15.487675   16.253452   15.776845\n",
    "  16.306961   41.61213    15.9031725  15.488239   19.056816   30.152964\n",
    "  18.068584   15.796449   15.773586   16.216064   17.080605   16.798647\n",
    "  16.733091   16.307858   38.06461    15.487675   16.69443    66.416145\n",
    "  16.733091   39.110504   15.266729   15.796449   16.401972   18.379236\n",
    "  15.929144   16.276924   15.929144   16.306961  360.43808    15.929144\n",
    "  20.011204   15.776845   15.796449   15.487675   39.291206   15.68768\n",
    "  15.488239   30.152964   16.879503   15.929144   16.306961   16.69443\n",
    "  16.401972   16.553946   15.796449   16.673416   17.080605  379.3062\n",
    "  16.745249   17.896193   16.90719    15.266729   15.776845   15.776845 ], \n",
    "  dtype=float32)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_density\n",
    "\n",
    "The `log_density` function first uses the effect handler `substitute` to return a callable which substitutes all primitive calls in `fn` with values from `data` whose key matches the site name. If the site name is not present in `data`, then there is no side effect. After `substitute`, another effect handler `trace` is used to record inputs, distributions, and outputs of `numpyro.sample` statements in the model, and NumPyro primitive calls, generally speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def log_density(model, model_args, model_kwargs, params):\n",
    "    \"\"\"\n",
    "    (EXPERIMENTAL INTERFACE) Computes log of joint density for the model given\n",
    "    latent values ``params``.\n",
    "\n",
    "    :param model: Python callable containing NumPyro primitives.\n",
    "    :param tuple model_args: args provided to the model.\n",
    "    :param dict model_kwargs: kwargs provided to the model.\n",
    "    :param dict params: dictionary of current parameter values keyed by site\n",
    "        name.\n",
    "    :return: log of joint density and a corresponding model trace\n",
    "    \"\"\"\n",
    "    \n",
    "    model = substitute(model, data=params)\n",
    "    model_trace = trace(model).get_trace(*model_args, **model_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect handlers allow us to effectively loop through each site in the model trace to compute the joint log probability density. In the for loop, if the site `type == sample` grab that sites value(s) (the samples from the numpyro.sample statement) and evaluate the log probability of the value(s) for that sites `fn` (dist.Normal(), dist.MultivariateNormal(), etc.) with `site['fn'].log_prob(<some value>)` The output snippet below shows the site `b1` defined in the model and the `value` sampled using the `numpyro.sample` statement. \n",
    "\n",
    "```\n",
    "site: {'type': 'sample', 'name': 'b1', 'fn': <numpyro.distributions.distribution.Independent object at 0x13a9ceb20>, 'args': (), 'kwargs': {'rng_key': None, 'sample_shape': ()}, 'value': Traced<ConcreteArray([ 1.2406311  -0.5222316  -1.2795658   1.800642   -0.43796206], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
    "  primal = Array([ 1.2406311 , -0.5222316 , -1.2795658 ,  1.800642  , -0.43796206],      dtype=float32)\n",
    "  tangent = Traced<ShapedArray(float32[5])>with<JaxprTrace(level=1/0)> with\n",
    "    pval = (ShapedArray(float32[5]), None)\n",
    "    recipe = LambdaBinding(), 'scale': None, 'is_observed': False, 'intermediates': [], 'cond_indep_stack': [], 'infer': {}} \n",
    "\n",
    "value: Traced<ConcreteArray([ 1.2406311  -0.5222316  -1.2795658   1.800642   -0.43796206], dtype=float32)>with<JVPTrace(level=2/0)> with\n",
    "  primal = Array([ 1.2406311 , -0.5222316 , -1.2795658 ,  1.800642  , -0.43796206],      dtype=float32)\n",
    "  tangent = Traced<ShapedArray(float32[5])>with<JaxprTrace(level=1/0)> with\n",
    "    pval = (ShapedArray(float32[5]), None)\n",
    "    recipe = LambdaBinding(), \n",
    "```\n",
    "\n",
    "Subsequently, we can also see the `fn` of this sample site defined in our model:\n",
    "\n",
    "```\n",
    "site fn: <numpyro.distributions.distribution.Independent object at 0x13a9ceb20>\n",
    "```\n",
    "\n",
    "where the `fn` is an `Independent` Normal distribution because we called the `.to_event(1)` method in our model. Next, the log probability for the sample site value is computed by calling the `.log_prob()` method. For example, the log probability of the sampled values for site `b1` is:\n",
    "\n",
    "```\n",
    "log prob.    = Traced<ConcreteArray(-8.036343574523926, dtype=float32)\n",
    "```\n",
    "\n",
    "Subsequently, we sum over the log probability for that site. Lastly, the variable `log_joint` is created for the log joint probability density, and the current site log probability is added to the log joint probability. After looping through each sample site, the log joint then represents the log joint probability density for the model given the latent values (parameters). \n",
    "\n",
    "```\n",
    "log joint         = Traced<ConcreteArray([ -15.773586   -15.796449   -15.68768    -17.106882   -16.307858\n",
    "  -15.10714    -16.277817   -16.401972   -17.409088   -15.488239\n",
    "  -17.44108    -20.011204   -15.796449   -16.401972   -17.409088\n",
    "  -15.488239   -32.46398    -16.306961   -18.321064   -15.776845\n",
    "  -16.432753   -41.972443   -16.90719    -17.324219   -15.487675\n",
    "  -15.68768    -15.10714    -15.68768    -16.96474    -61.152782\n",
    "  -16.319      -32.46398    -56.631355   -16.733091   -44.390583\n",
    "  -15.796449   -31.771408   -15.9031725  -17.999393   -15.929144\n",
    "  -15.796449   -15.776845   -25.234818   -15.487675   -32.46398\n",
    "  -15.776845   -22.136528   -16.745249   -15.796449   -15.796449\n",
    "  -61.152782   -17.459694   -15.776845   -39.30664    -31.771408\n",
    "  -17.106882   -15.796449   -15.776845   -16.836826   -16.90372\n",
    "  -15.565512   -15.266311   -15.796449   -15.487675   -25.503807\n",
    "  -66.416145   -42.01054    -15.68768    -16.438005   -35.528217\n",
    "  -16.401972  -275.8356     -15.488239   -46.813717   -18.31475\n",
    "  -42.01054    -15.929144   -16.733091   -15.929144   -18.632296\n",
    "  -16.553946   -22.139755   -16.879503   -16.253452   -15.929144\n",
    "  -16.68712    -16.90719    -17.409088   -16.306961   -17.900412\n",
    "  -72.883484   -20.984446   -17.080605   -15.68768    -15.266311\n",
    "  -17.459694   -15.487675   -17.409088  -221.04407    -15.487675\n",
    "  -15.68768    -15.68768    -15.903938   -17.608683   -17.233418\n",
    "  -16.945618   -17.102604   -16.230682   -16.401972   -20.437622\n",
    "  -16.307858   -15.776845   -66.416145   -22.85551    -17.459694\n",
    "  -15.266729   -18.263693   -16.733091   -15.68768    -16.69443\n",
    "  -17.767635   -16.892221   -16.277817   -16.699389   -15.796449\n",
    "  -15.776845   -15.68768    -17.459694   -18.93738    -16.401972\n",
    "  -41.471767   -15.796449   -82.16476    -16.664154   -15.68768\n",
    "  -17.409088   -17.106882   -15.488239   -15.266729   -17.917303\n",
    "  -26.629543   -21.383934   -18.279554   -15.929144   -16.90719\n",
    "  -38.06461    -16.673416   -15.487675   -16.253452   -15.776845\n",
    "  -16.306961   -41.61213    -15.9031725  -15.488239   -19.056816\n",
    "  -30.152964   -18.068584   -15.796449   -15.773586   -16.216064\n",
    "  -17.080605   -16.798647   -16.733091   -16.307858   -38.06461\n",
    "  -15.487675   -16.69443    -66.416145   -16.733091   -39.110504\n",
    "  -15.266729   -15.796449   -16.401972   -18.379236   -15.929144\n",
    "  -16.276924   -15.929144   -16.306961  -360.43808    -15.929144\n",
    "  -20.011204   -15.776845   -15.796449   -15.487675   -39.291206\n",
    "  -15.68768    -15.488239   -30.152964   -16.879503   -15.929144\n",
    "  -16.306961   -16.69443    -16.401972   -16.553946   -15.796449\n",
    "  -16.673416   -17.080605  -379.3062     -16.745249   -17.896193\n",
    "  -16.90719    -15.266729   -15.776845   -15.776845 ], dtype=float32)\n",
    "```\n",
    "\n",
    "And voila, this output is the log joint probability density (and placing a negative sign in front of this array gives you the potential energy) for the model given the latent values. The values in the output above represent the log joint probability of the initialized latent valus, i.e., no inference has been ran yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def log_density(model, model_args, model_kwargs, params):\n",
    "    \"\"\"\n",
    "    (EXPERIMENTAL INTERFACE) Computes log of joint density for the model given\n",
    "    latent values ``params``.\n",
    "\n",
    "    :param model: Python callable containing NumPyro primitives.\n",
    "    :param tuple model_args: args provided to the model.\n",
    "    :param dict model_kwargs: kwargs provided to the model.\n",
    "    :param dict params: dictionary of current parameter values keyed by site\n",
    "        name.\n",
    "    :return: log of joint density and a corresponding model trace\n",
    "    \"\"\"\n",
    "    \n",
    "    model = substitute(model, data=params)\n",
    "    model_trace = trace(model).get_trace(*model_args, **model_kwargs)\n",
    "    log_joint = jnp.zeros(())\n",
    "    print(\"---- inside log_density -----\")\n",
    "    print('\\n')\n",
    "    for site in model_trace.values():\n",
    "        print(f\"site: {site}\", '\\n')\n",
    "        if site[\"type\"] == \"sample\":\n",
    "            value = site[\"value\"]\n",
    "            print(f\"value: {value}, \\n\")\n",
    "            intermediates = site[\"intermediates\"]\n",
    "            print(f\"intermediates: {intermediates}, \\n\")\n",
    "            scale = site[\"scale\"]\n",
    "            print(f\"site fn: {site['fn']}, \\n\")\n",
    "            if intermediates:\n",
    "                log_prob = site[\"fn\"].log_prob(value, intermediates)\n",
    "            else:\n",
    "                guide_shape = jnp.shape(value)\n",
    "                model_shape = tuple(\n",
    "                    site[\"fn\"].shape()\n",
    "                )  # TensorShape from tfp needs casting to tuple\n",
    "                try:\n",
    "                    broadcast_shapes(guide_shape, model_shape)\n",
    "                except ValueError:\n",
    "                    raise ValueError(\n",
    "                        \"Model and guide shapes disagree at site: '{}': {} vs {}\".format(\n",
    "                            site[\"name\"], model_shape, guide_shape\n",
    "                        )\n",
    "                    )\n",
    "                log_prob = site[\"fn\"].log_prob(value)\n",
    "\n",
    "            if (scale is not None) and (not is_identically_one(scale)):\n",
    "                log_prob = scale * log_prob\n",
    "\n",
    "            # print(f\"before sum log prob.    = {log_prob}, \\n\")\n",
    "            # log_prob = jnp.sum(log_prob)\n",
    "            # print(f\"after sum log prob.     = {log_prob}, \\n\")\n",
    "            log_joint = log_joint + log_prob\n",
    "            print(f\"log joint               = {log_joint}, \\n\")\n",
    "    return log_joint, model_trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning to get_potential_fn\n",
    "\n",
    "However, `log_density` and `potential_energy` compute the log probability of the current latent value, not a **function**. Therefore, we return to the Python function `get_potential_fn` that returns the log joint probability as a function `potential_fn`, i.e., a function that will evaluate the potential energy given the model args defined by our `model`, i.e., `X_train` and `y_train` and the latent values using the `log_density` function described above. The `potential_fn` is then \"fed\" into the HMC algorithm to perform inference.\n",
    "\n",
    "What's great about `get_potential_fn` is that the log joint probability density function can be accessed externally given our NumPyro model, and passed into other sampling libraries such as [Blackjax](https://blackjax-devs.github.io/blackjax/examples/howto_use_numpyro.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "To translate a NumPyro model, in the form of a Python function, to a joint probability (function and value) a series of function calls are performed. Namely, initialize model $\\leftrightarrow$ get potential fn $\\leftrightarrow$ potential energy $\\leftrightarrow$ log density. \n",
    "\n",
    "- `log_density` computes the log probability of the current latent (parameter) value and observed data\n",
    "- `potential_energy` is `-log_density`\n",
    "- `get_potential_fn` returns the log joint probability as a function `potential_fn`, i.e., a function that will evaluate the potential energy given the model args defined by our `model`, i.e., `X_train` and `y_train` and the latent values using the `log_density` function. The `potential_fn` is then \"fed\" into the HMC algorithm to perform inference.\n",
    "\n",
    "This process allows users to (1) translate their models in a dynamic manner, and (2) access the log joint probability for external use, such as in Blackjax."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
